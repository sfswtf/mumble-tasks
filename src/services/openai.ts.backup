import OpenAI from 'openai';
import { BiographyContent } from '../types';

const openai = new OpenAI({
  apiKey: import.meta.env.VITE_OPENAI_API_KEY,
  dangerouslyAllowBrowser: true
});

// Approximate token count (rough estimate: 1 token ≈ 4 characters)
function estimateTokenCount(text: string): number {
  return Math.ceil(text.length / 4);
}

// Split text into chunks that fit within token limit
function chunkText(text: string, maxTokens: number = 15000): string[] {
  const chunks: string[] = [];
  let currentChunk = '';
  const sentences = text.split(/(?<=[.!?])\s+/);
  
  for (const sentence of sentences) {
    const sentenceTokens = estimateTokenCount(sentence);
    const currentChunkTokens = estimateTokenCount(currentChunk);
    
    if (currentChunkTokens + sentenceTokens > maxTokens && currentChunk.length > 0) {
      chunks.push(currentChunk.trim());
      currentChunk = sentence;
    } else {
      currentChunk += (currentChunk ? ' ' : '') + sentence;
    }
  }
  
  if (currentChunk.trim().length > 0) {
    chunks.push(currentChunk.trim());
  }
  
  return chunks;
}

export async function transcribeAudio(
  audioFile: File, 
  language: string = 'en', 
  onProgress?: (progress: number) => void
): Promise<{ text: string }> {
  try {
    // Report initial progress
    onProgress?.(0);
    
    const formData = new FormData();
    formData.append('file', audioFile);
    formData.append('model', 'whisper-1');
    formData.append('language', language);
    formData.append('response_format', 'json');

    // Report progress for request preparation
    onProgress?.(0.2);

    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${import.meta.env.VITE_OPENAI_API_KEY}`,
      },
      body: formData,
    });

    // Report progress for response processing
    onProgress?.(0.8);

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      console.error('OpenAI API Error Response:', errorData);
      throw new Error(errorData.error?.message || `Transcription failed: ${response.statusText}`);
    }

    const data = await response.json();
    if (!data.text) {
      throw new Error('No transcription text received from API');
    }

    // Report completion
    onProgress?.(1);

    return { text: data.text };
  } catch (error) {
    console.error('Transcription error:', error);
    throw error instanceof Error 
      ? error 
      : new Error('Failed to transcribe audio. Please try again.');
  }
}

export async function generateContent(
  text: string, 
  preferences: any, 
  language: string = 'en'
): Promise<BiographyContent> {
  try {
    // Check if text is too long and needs chunking
    const tokenCount = estimateTokenCount(text);
    if (tokenCount > 15000) {
      console.log(`Text is ${tokenCount} tokens long, chunking into smaller pieces`);
      const chunks = chunkText(text);
      console.log(`Split into ${chunks.length} chunks`);
      
      // Process each chunk and combine results
      const results = await Promise.all(
        chunks.map(async (chunk, index) => {
          console.log(`Processing chunk ${index + 1}/${chunks.length}`);
          return processChunk(chunk, preferences, language);
        })
      );
      
      // Combine results
      const combinedContent = results.map(r => r.content).join('\n\n');
      return {
        content: combinedContent,
        type: preferences.promptType || 'default',
        language
      };
    } else {
      return processChunk(text, preferences, language);
    }
  } catch (error) {
    console.error('Content generation error:', error);
    throw error instanceof Error 
      ? error 
      : new Error('Failed to generate content. Please try again.');
  }
}

async function processChunk(
  text: string, 
  preferences: any, 
  language: string
): Promise<BiographyContent> {
  const prompt = buildPrompt(text, preferences, language);
  
  const response = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: [
      { role: "system", content: "You are a helpful assistant that processes text according to user preferences." },
      { role: "user", content: prompt }
    ],
    temperature: 0.7,
    max_tokens: 4000
  });
  
  const content = response.choices[0]?.message?.content || '';
  
  return {
    content,
    type: preferences.promptType || 'default',
    language
  };
}

function buildPrompt(text: string, preferences: any, language: string): string {
  // Build a prompt based on preferences
  let prompt = `Process the following text according to these preferences:\n\n`;
  
  if (preferences.tone) {
    prompt += `Tone: ${preferences.tone}\n`;
  }
  
  if (preferences.style) {
    prompt += `Style: ${preferences.style}\n`;
  }
  
  if (preferences.audience) {
    prompt += `Audience: ${preferences.audience}\n`;
  }
  
  if (preferences.notes) {
    prompt += `Additional notes: ${preferences.notes}\n`;
  }
  
  if (preferences.authorGenre) {
    prompt += `Genre: ${preferences.authorGenre}\n`;
  }
  
  if (preferences.authorStyle) {
    prompt += `Writing style: ${preferences.authorStyle}\n`;
  }
  
  if (preferences.authorContext) {
    prompt += `Context: ${preferences.authorContext}\n`;
  }
  
  if (preferences.authorInstructions) {
    prompt += `Instructions: ${preferences.authorInstructions}\n`;
  }
  
  if (preferences.authorPasteText) {
    prompt += `Continue from this text: ${preferences.authorPasteText}\n\n`;
  }
  
  prompt += `\nText to process:\n${text}\n\n`;
  prompt += `Please process this text according to the preferences above. Do not add introductions, conclusions, or section headers unless specifically requested.`;
  
  return prompt;
}

export async function generateSummaryAndTasks(
  transcription: string,
  language: string = 'en',
  onProgress?: (progress: number) => void
): Promise<{ summary: string; tasks: string[] }> {
  if (!transcription?.trim()) {
    throw new Error('No transcription provided for summary generation');
  }

  try {
    // Report initial progress
    onProgress?.(0);

    const systemPrompt = language === 'no'
      ? `Du er en hjelpsom assistent som analyserer talenotater og lager omfattende sammendrag. For følgende transkripsjon, gi:
        1. Et detaljert sammendrag som fanger opp alle viktige punkter og detaljer fra møtet/notatet
        2. En komplett liste over alle spesifikke, gjennomførbare oppgaver som nevnes eller antydes
        
        Sammendrag skal være omfattende og inkludere:
        - Hovedpunkter og beslutninger
        - Viktige detaljer og kontekst
        - Nøkkelpersoner og deres roller
        - Tidspunkt og frister som nevnes
        
        Formater svaret slik:
        Sammendrag: [detaljert sammendragstekst]
        Oppgaver:
        - [oppgave 1]
        - [oppgave 2]
        osv.`
      : `You are a helpful assistant that analyzes voice memos and creates comprehensive summaries. For the following transcription, provide:
        1. A detailed executive summary that captures all important points and details from the meeting/memo
        2. A complete list of all specific, actionable tasks mentioned or implied
        
        The summary should be comprehensive and include:
        - Key points and decisions made
        - Important details and context
        - Key people and their roles
        - Timelines and deadlines mentioned
        
        Format the response as:
        Summary: [detailed summary text]
        Tasks:
        - [task 1]
        - [task 2]
        etc.`;

    // Report progress for API call preparation
    onProgress?.(0.3);

    const completion = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "system",
          content: systemPrompt
        },
        {
          role: "user",
          content: transcription
        }
      ],
      temperature: 0.7,
      max_tokens: 2000 // Increased for more comprehensive summaries
    });

    // Report progress for response processing
    onProgress?.(0.8);

    const response = completion.choices[0]?.message?.content;
    if (!response) {
      throw new Error('No response received from OpenAI');
    }

    // Parse response based on language
    const summaryLabel = language === 'no' ? 'Sammendrag:' : 'Summary:';
    const tasksLabel = language === 'no' ? 'Oppgaver:' : 'Tasks:';

    const [summaryPart, tasksPart] = response.split(new RegExp(`\n${tasksLabel}\n`, 'i'));
    const summary = summaryPart.replace(new RegExp(`^${summaryLabel}\\s*`, 'i'), '').trim();
    const tasks = tasksPart
      ? tasksPart.split('\n')
          .filter(task => task.trim().startsWith('-'))
          .map(task => task.replace(/^-\s*/, '').trim())
      : [];

    if (!summary) {
      throw new Error('Failed to generate summary from the transcription');
    }

    // Report completion
    onProgress?.(1);

    return { summary, tasks: tasks.length ? tasks : [] };
  } catch (error) {
    console.error('Summary and tasks generation error:', error);
    throw error instanceof Error 
      ? error 
      : new Error('Failed to generate summary and tasks. Please try again.');
  }
}

export async function generatePromptContent(
  transcription: string,
  type: string,
  customization: any,
  onProgress?: (progress: number) => void
): Promise<BiographyContent> {
  // Report initial progress
  onProgress?.(0);

  let systemPrompt = '';
  
  switch (type) {
    case 'prompt':
      if (customization.promptMode === 'initial') {
        systemPrompt = `You are a prompt engineering expert. Based on the user's description, create 3 different variations of clear, effective prompts for ${customization.promptType}. 
        
        Format the output exactly like this:

        1. [First prompt variation]

        2. [Second prompt variation]

        3. [Third prompt variation]

        Tips for Best Results:
        • [Tip 1]
        • [Tip 2]
        • [Tip 3]`;
      } else {
        systemPrompt = `You are a prompt engineering expert helping users refine their interactions with AI models. 
        Analyze the LLM's output and the user's question/concern, then provide:

        1. A clear, refined response to address the LLM's questions
        2. Suggestions for additional context or clarifications that could improve the response
        3. Alternative approaches to consider

        Original LLM Output:
        ${customization.llmOutput}

        User's Question/Concern:
        ${customization.notes}`;
      }
      break;
      
    case 'meeting':
      systemPrompt = `You are a professional meeting notes expert. Analyze the meeting recording and extract:
      
      1. Key Discussion Points
      - Main topics covered
      - Important insights shared
      
      2. Decisions Made
      - List all decisions and agreements
      - Include context for each decision
      
      3. Action Items
      - Specific tasks assigned
      - Responsible parties
      - Deadlines if mentioned
      
      4. Follow-up Items
      - Questions to be answered
      - Topics for next meeting
      - Required preparations`;
      break;
      
    case 'tasks':
      systemPrompt = `You are a task organization expert. Convert the following content into a structured ${customization.taskType} list.
      For each task, include:
      - Clear, actionable description
      - Priority level
      - Estimated time/effort
      - Any dependencies or prerequisites`;
      break;
      
    default:
      systemPrompt = `Convert the following content into a well-structured ${type} format.
      Consider the following preferences:
      - Tone: ${customization.tone}
      - Style: ${customization.style}
      - Target Audience: ${customization.audience}
      Additional Notes: ${customization.notes}`;
  }

  try {
    // Report progress for API call preparation
    onProgress?.(0.3);

    const completion = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "system",
          content: systemPrompt
        },
        {
          role: "user",
          content: transcription
        }
      ],
      temperature: 0.7,
      max_tokens: 2000
    });

    // Report progress for response processing
    onProgress?.(0.8);

    const content = completion.choices[0]?.message?.content;
    if (!content) {
      throw new Error('No content generated');
    }

    // Report completion
    onProgress?.(1);

    return {
      content,
      type,
      language: 'en'
    };
  } catch (error) {
    console.error('Content generation error:', error);
    throw error instanceof Error 
      ? error 
      : new Error('Failed to generate content. Please try again.');
  }
}